import os
import shutil
import json
from datetime import datetime
from janome.tokenizer import Tokenizer
from collections import Counter, defaultdict
import networkx as nx
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib import rcParams
from matplotlib.colors import Normalize
import matplotlib.cm as cm
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email.mime.text import MIMEText
from email import encoders
from gensim import corpora, models
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA, LatentDirichletAllocation
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import numpy as np
import pandas as pd
from wordcloud import WordCloud
import MeCab
from textstat import flesch_reading_ease
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
rcParams['font.family'] = 'IPAGothic'
plt.rcParams['font.size'] = 12
sns.set_style("whitegrid")
sns.set_palette("husl")

class AdvancedTextMiner:
    """é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ¨è«–ãƒ»æ„Ÿæƒ…èªãƒ»æ§‹é€ èªé™¤å»æ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, config_path=None):
        # è¨­å®šã®åˆæœŸåŒ–
        self.config = self._load_config(config_path) if config_path else self._default_config()
        self.tokenizer = Tokenizer()
        
        # æ”¹è‰¯ç‰ˆï¼šè¨€èªå­¦çš„ã‚«ãƒ†ã‚´ãƒªåˆ¥é™¤å¤–èªè¾æ›¸ã®åˆæœŸåŒ–
        self._init_linguistic_filters()
        
        # MeCabã®è¨­å®šã¨è©³ç´°ãªçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
        self.use_mecab = False
        self.mecab = None
        self._setup_mecab()
            
        self.results = {}
    
    def _init_linguistic_filters(self):
        """è¨€èªå­¦çš„ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é™¤å¤–èªè¾æ›¸ã‚’åˆæœŸåŒ–"""
        
        # 1. æ¨è«–ãƒ»æ„Ÿæƒ…èªã‚«ãƒ†ã‚´ãƒªï¼ˆèªçŸ¥ãƒ»æ„Ÿæƒ…ãƒ»æ¨æ¸¬ã‚’è¡¨ã™èªå½™ï¼‰
        self.inference_emotion_words = {
            # æ¨è«–ãƒ»æ¨æ¸¬å‹•è©
            'ãŠã‚‚ã†', 'æ€ã†', 'ã‹ã‚“ãŒãˆã‚‹', 'è€ƒãˆã‚‹', 'ã—ã‚“ã˜ã‚‹', 'ä¿¡ã˜ã‚‹', 
            'ã™ã„ãã', 'æ¨æ¸¬', 'ã™ã„ã‚ã‚“', 'æ¨è«–', 'ã™ã„ã¦ã„', 'æ¨å®š',
            'ã‚ˆãã†', 'äºˆæƒ³', 'ããŸã„', 'æœŸå¾…', 'ã‚Šã‹ã„', 'ç†è§£',
            
            # å¯èƒ½æ€§ãƒ»æ¨é‡è¡¨ç¾ï¼ˆæ§˜æ…‹è¡¨ç¾ã‚’å¼·åŒ–ï¼‰
            'ã—ã‚Œã‚‹', 'ã‹ã‚‚ã—ã‚Œãªã„', 'ã ã‚ã†', 'ã§ã‚ã‚ã†', 'ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“',
            'ã‚‰ã—ã„', 'ã‚ˆã†ã ', 'ã‚ˆã†ãª', 'ã‚ˆã†ã«', 'ã‚ˆã†', 'ã¿ãŸã„', 'ã¿ãŸã„ãª',
            'ã£ã½ã„', 'ã’', 'ãŒã¡', 'ãã†', 'ãã†ã ', 'ãã†ãª',
            
            # æ„Ÿæƒ…å‹•è©
            'ã‹ã‚“ã˜ã‚‹', 'æ„Ÿã˜ã‚‹', 'ã‚ˆã‚ã“ã¶', 'å–œã¶', 'ã‹ãªã—ã‚€', 'æ‚²ã—ã‚€',
            'ãŠã“ã‚‹', 'æ€’ã‚‹', 'ãŠã©ã‚ã', 'é©šã', 'ã—ã‚“ã±ã„', 'å¿ƒé…',
            'ã‚ã‚“ã—ã‚“', 'å®‰å¿ƒ', 'ã“ã†ã‹ã„', 'å¾Œæ‚”', 'ã¾ã‚“ãã', 'æº€è¶³',
            
            # æ„Ÿæƒ…å½¢å®¹è©
            'ã†ã‚Œã—ã„', 'å¬‰ã—ã„', 'ã‹ãªã—ã„', 'æ‚²ã—ã„', 'ãŸã®ã—ã„', 'æ¥½ã—ã„',
            'ã¤ã‚‰ã„', 'ã—ã‚“ã©ã„', 'ãã‚‚ã¡ã„', 'æ°—æŒã¡ã„', 'ã„ã‚„ã ', 'å«Œã ',
            
            # è©•ä¾¡ãƒ»åˆ¤æ–­èª
            'ã²ã‚‡ã†ã‹', 'è©•ä¾¡', 'ã¯ã‚“ã ã‚“', 'åˆ¤æ–­', 'ã„ã‘ã‚“', 'æ„è¦‹',
            'ã‹ã‚“ãã†', 'æ„Ÿæƒ³', 'ã„ã‚“ã—ã‚‡ã†', 'å°è±¡', 'ã‹ã‚“ã¦ã‚“', 'è¦³ç‚¹'
        }
        
        # 2. æ§‹é€ èªã‚«ãƒ†ã‚´ãƒªï¼ˆæ–‡ç« æ§‹é€ ã‚„è«–ç†é–¢ä¿‚ã‚’ç¤ºã™èªå½™ï¼‰
        self.structural_words = {
            # æ¥ç¶šè©ãƒ»æ¥ç¶šå‰¯è©
            'ã—ã‹ã—', 'ã ãŒ', 'ã‘ã‚Œã©', 'ã‘ã‚Œã©ã‚‚', 'ã¨ã“ã‚ãŒ', 'ã§ã‚‚',
            'ãã—ã¦', 'ãã‚Œã‹ã‚‰', 'ã¤ãã«', 'æ¬¡ã«', 'ã•ã‚‰ã«', 'ã¾ãŸ',
            'ã ã‹ã‚‰', 'ãã‚Œã§', 'ã‚†ãˆã«', 'ã—ãŸãŒã£ã¦', 'ãã®ãŸã‚',
            'ã¤ã¾ã‚Š', 'ã™ãªã‚ã¡', 'ã‚ˆã†ã™ã‚‹ã«', 'è¦ã™ã‚‹ã«', 'ã„ã„ã‹ãˆã‚Œã°', 'è¨€ã„æ›ãˆã‚Œã°',
            'ãŸã¨ãˆã°', 'ä¾‹ãˆã°', 'ã¡ãªã¿ã«', 'ã¨ã“ã‚ã§', 'ã•ã¦',
            'ãŸã ã—', 'ã‚‚ã£ã¨ã‚‚', 'ãªãŠ', 'ã¡ãªã¿ã«', 'ã„ã£ã½ã†', 'ä¸€æ–¹',
            
            # æ–‡ç« æ§‹é€ èª
            'ã¯ã˜ã‚ã«', 'åˆã‚ã«', 'ã¤ãã«', 'æ¬¡ã«', 'ã•ã„ã”ã«', 'æœ€å¾Œã«',
            'ã‘ã£ã‚ã‚“', 'çµè«–', 'ã‚ˆã†ã‚„ã', 'ã¾ã¨ã‚', 'ã›ã¤ã‚ã„', 'èª¬æ˜',
            'ã‚Šã‚†ã†', 'ç†ç”±', 'ã’ã‚“ã„ã‚“', 'åŸå› ', 'ã‘ã£ã‹', 'çµæœ',
            'ã‚‚ãã¦ã', 'ç›®çš„', 'ã»ã†ã»ã†', 'æ–¹æ³•', 'ã—ã‚…ã ã‚“', 'æ‰‹æ®µ',
            
            # ç¨‹åº¦ãƒ»é »åº¦å‰¯è©
            'ã¨ã¦ã‚‚', 'ã‹ãªã‚Š', 'ãšã„ã¶ã‚“', 'ã ã„ã¶', 'ã‚ã‚Šã¨', 'ã‘ã£ã“ã†',
            'ã™ã“ã—', 'å°‘ã—', 'ã¡ã‚‡ã£ã¨', 'ã‚„ã‚„', 'ã‚ãšã‹', 'ã»ã‚“ã®',
            'ãŸã„ã¸ã‚“', 'å¤§å¤‰', 'ã²ã˜ã‚‡ã†', 'éå¸¸', 'ãã‚ã‚ã¦', 'æ¥µã‚ã¦',
            'ã„ã¤ã‚‚', 'å¸¸ã«', 'ã‚ˆã', 'ã¨ãã©ã', 'æ™‚ã€…', 'ãŸã¾ã«',
            'ãœã‚“ãœã‚“', 'å…¨ç„¶', 'ã¾ã£ãŸã', 'å…¨ã', 'ã‘ã£ã—ã¦', 'æ±ºã—ã¦',
            
            # æ™‚é–“ãƒ»ç©ºé–“æŒ‡ç¤ºèª
            'ã„ã¾', 'ä»Š', 'ãã‚‡ã†', 'ä»Šæ—¥', 'ãã®ã†', 'æ˜¨æ—¥', 'ã‚ã—ãŸ', 'æ˜æ—¥',
            'ã“ã“', 'ãã“', 'ã‚ãã“', 'ã†ãˆ', 'ä¸Š', 'ã—ãŸ', 'ä¸‹', 'ã¾ãˆ', 'å‰',
            'ã¾ã‚ã‚Š', 'å‘¨ã‚Š', 'ã‚ã„ã ', 'é–“', 'ãªã‹', 'ä¸­', 'ãã¨', 'å¤–',
            
            # æ•¬èªãƒ»ä¸å¯§èªï¼ˆæ‹¡å¼µï¼‰
            'ã„ã‚‰ã£ã—ã‚ƒã‚‹', 'ãã ã•ã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 'ã•ã›ã‚‹',
            'ã§ã™', 'ã¾ã™', 'ã§ã‚ã‚Šã¾ã™', 'ã”ã–ã„ã¾ã™', 'ã„ãŸã—ã¾ã™'
        }
        
        # 3. æ©Ÿèƒ½èªã‚«ãƒ†ã‚´ãƒªï¼ˆæ‹¡å¼µç‰ˆï¼‰
        self.functional_words = {
            # åŸºæœ¬çš„ãªæ©Ÿèƒ½èª
            'ã™ã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ãã ã•ã‚‹', 'ãã ã•ã„', 'ã§ã‚ã‚‹', 'ã ', 'ã§ã™', 'ã¾ã™',
            
            # å½¢å¼åè©ï¼ˆæ‹¡å¼µï¼‰
            'ã“ã¨', 'ã‚‚ã®', 'ã¨ã“ã‚', 'ãŸã‚', 'ã‚ã‘', 'ã¯ãš', 'ã¤ã‚‚ã‚Š', 'ã»ã†', 'ã†ã¡', 
            'ã¨ã', 'æ™‚', 'ã°ã‚ã„', 'å ´åˆ', 'ã¨ãŠã‚Š', 'é€šã‚Š', 'ãŸã³', 'åº¦',
            'ã•ã„', 'éš›', 'ã‚ã„ã ', 'é–“', 'ã¾ãˆ', 'å‰', 'ã‚ã¨', 'å¾Œ',
            
            # ä»£åè©ï¼ˆæ‹¡å¼µï¼‰
            'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã“ã®', 'ãã®', 'ã‚ã®', 'ã“ã“', 'ãã“', 'ã‚ãã“',
            'ã©ã‚Œ', 'ã©ã®', 'ã©ã“', 'ã ã‚Œ', 'èª°', 'ãªã«', 'ä½•', 'ã„ã¤', 'ãªãœ', 'ã©ã†',
            
            # ä¸€èˆ¬çš„ã™ãã‚‹å‹•è©ï¼ˆæ‹¡å¼µï¼‰
            'ã§ãã‚‹', 'ã¿ã‚‹', 'è¦‹ã‚‹', 'ã„ã', 'è¡Œã', 'ãã‚‹', 'æ¥ã‚‹', 'ãŠã', 'ç½®ã', 'ã‚„ã‚‹',
            'ã‚‚ã¤', 'æŒã¤', 'ã„ã†', 'è¨€ã†', 'ãã', 'èã', 'ã‹ã', 'æ›¸ã', 'ã‚ˆã‚€', 'èª­ã‚€',
            'ãŸã¹ã‚‹', 'é£Ÿã¹ã‚‹', 'ã®ã‚€', 'é£²ã‚€', 'ã­ã‚‹', 'å¯ã‚‹', 'ãŠãã‚‹', 'èµ·ãã‚‹',
            'ã¯ãŸã‚‰ã', 'åƒã', 'ã¹ã‚“ãã‚‡ã†', 'å‹‰å¼·', 'ã—ã”ã¨', 'ä»•äº‹',
            
            # åŠ©æ•°è©çš„ãªã‚‚ã®ï¼ˆæ‹¡å¼µï¼‰
            'å›', 'åº¦', 'ä»¶', 'å€‹', 'äºº', 'å', 'æœ¬', 'å†Š', 'æš', 'å°', 'æ©Ÿ', 'ç¤¾', 'æ ¡',
            'æ—¥', 'æœˆ', 'å¹´', 'æ™‚é–“', 'åˆ†', 'ç§’', 'ã¤', 'ã‹æœˆ', 'ã°ã‚“', 'ç•ª'
        }
        
        # 4. æ•¬ç§°ãƒ»æ•¬èªã‚«ãƒ†ã‚´ãƒªï¼ˆæ–°è¨­ãƒ»å¼·åŒ–ï¼‰
        self.honorific_words = {
            # æ•¬ç§°ï¼ˆäººåã«ä»˜ãï¼‰
            'ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“', 'æ§˜', 'ã•ã¾', 'æ°', 'å›', 'ã•ã¾', 'ã¯ã‚“',
            
            # æ•¬èªãƒ»ä¸å¯§èªï¼ˆæ‹¡å¼µï¼‰
            'ã„ã‚‰ã£ã—ã‚ƒã‚‹', 'ãã ã•ã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã›ã‚‹', 'ã•ã›ã‚‹',
            'ã§ã™', 'ã¾ã™', 'ã§ã‚ã‚Šã¾ã™', 'ã”ã–ã„ã¾ã™', 'ã„ãŸã—ã¾ã™',
            'ã—ã¦ã„ãŸã ã', 'ã•ã›ã¦ã„ãŸã ã', 'ãŠã‚Šã¾ã™', 'ã„ã‚‰ã£ã—ã‚ƒã„ã¾ã™',
            
            # è¬™è­²èª
            'ã„ãŸã™', 'ã•ã›ã¦ã„ãŸã ã', 'ãŠã†ã‹ãŒã„', 'ãŠä¼ºã„', 'ãŠã˜ã‚ƒã¾', 'ãŠé‚ªé­”',
            'ã†ã‹ãŒã†', 'ä¼ºã†', 'ã‚‚ã†ã™', 'ç”³ã™', 'ã‚‚ã†ã—ã‚ã’ã‚‹', 'ç”³ã—ä¸Šã’ã‚‹'
        }
        
        # 5. çµ±åˆé™¤å¤–èªã‚»ãƒƒãƒˆã®ä½œæˆ
        self.all_excluded_words = (
            self.inference_emotion_words | 
            self.structural_words | 
            self.functional_words |
            self.honorific_words  # æ•¬ç§°ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ 
        )
        
        print(f"é™¤å¤–èªè¾æ›¸ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ:")
        print(f"ãƒ»æ¨è«–ãƒ»æ„Ÿæƒ…èª: {len(self.inference_emotion_words)}èª")
        print(f"ãƒ»æ§‹é€ èª: {len(self.structural_words)}èª")
        print(f"ãƒ»æ©Ÿèƒ½èª: {len(self.functional_words)}èª")
        print(f"ãƒ»æ•¬ç§°ãƒ»æ•¬èª: {len(self.honorific_words)}èª")  # æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒª
        print(f"ãƒ»ç·é™¤å¤–èªæ•°: {len(self.all_excluded_words)}èª")
    
    def _setup_mecab(self):
        """MeCabã®è©³ç´°ãªè¨­å®šã¨çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        print("\nå½¢æ…‹ç´ è§£æã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®šã‚’ç¢ºèªä¸­...")
        
        # Step 1: MeCabãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
        try:
            import MeCab
            print("âœ“ MeCabãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        except ImportError as e:
            print("âŒ MeCabãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print(f"   è©³ç´°: {e}")
            print("   â†’ Janomeã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆååˆ†ãªæ€§èƒ½ãŒã‚ã‚Šã¾ã™ï¼‰")
            return
        
        # Step 2: MeCabã‚¿ã‚¬ãƒ¼ã®åˆæœŸåŒ–ç¢ºèª
        try:
            # ä¸€èˆ¬çš„ãªè¨­å®šã§ã®åˆæœŸåŒ–ã‚’è©¦è¡Œ
            self.mecab = MeCab.Tagger('-Ochasen')
            print("âœ“ MeCabã‚¿ã‚¬ãƒ¼ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
        except Exception as e:
            print("âŒ MeCabã‚¿ã‚¬ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            print(f"   è©³ç´°: {e}")
            
            # è¾æ›¸ãƒ‘ã‚¹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦å†è©¦è¡Œ
            try:
                self.mecab = MeCab.Tagger('-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/ipadic-utf8')
                print("âœ“ è¾æ›¸ãƒ‘ã‚¹æŒ‡å®šã§MeCabã‚¿ã‚¬ãƒ¼ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
            except Exception as e2:
                print(f"âŒ è¾æ›¸ãƒ‘ã‚¹æŒ‡å®šã§ã‚‚å¤±æ•—: {e2}")
                print("   â†’ Janomeã‚’ä½¿ç”¨ã—ã¾ã™")
                return
        
        # Step 3: å®Ÿéš›ã®å‹•ä½œãƒ†ã‚¹ãƒˆ
        try:
            test_result = self.mecab.parse("ãƒ†ã‚¹ãƒˆæ–‡ç« ã§ã™ã€‚")
            if test_result and len(test_result.strip()) > 0:
                print("âœ“ MeCabå‹•ä½œãƒ†ã‚¹ãƒˆã«æˆåŠŸã—ã¾ã—ãŸ")
                print(f"   ãƒ†ã‚¹ãƒˆçµæœ: {test_result.strip()[:50]}...")
                self.use_mecab = True
                print("ğŸ‰ MeCabãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼é«˜ç²¾åº¦ãªè§£æã‚’ä½¿ç”¨ã—ã¾ã™")
            else:
                print("âŒ MeCabå‹•ä½œãƒ†ã‚¹ãƒˆã§ç©ºã®çµæœãŒè¿”ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ MeCabå‹•ä½œãƒ†ã‚¹ãƒˆã«å¤±æ•—: {e}")
            print("   â†’ Janomeã‚’ä½¿ç”¨ã—ã¾ã™")
        
        if not self.use_mecab:
            print("\nğŸ“ MeCabã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’è©¦ã—ã¦ãã ã•ã„:")
            print("   sudo apt-get install python3-dev build-essential libmecab-dev")
            print("   pip install mecab-python3")
            print("\nğŸ’¡ ç¾åœ¨ã¯Janomeã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸€èˆ¬çš„ãªç”¨é€”ã«ã¯ååˆ†ãªæ€§èƒ½ã§ã™ï¼")
    
    def _default_config(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆæ”¹è‰¯ç‰ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ãï¼‰"""
        return {
            'source_dir': os.path.expanduser('~/Dropbox/text'),
            'archive_dir': os.path.expanduser('~/Dropbox/processed'),
            'output_dir': os.path.expanduser('~/Dropbox/results'),
            'from_email': "ã‚ãªãŸã®Gmailã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å…¥åŠ›ã—ã¦ä¸‹ã•ã„",
            'to_email': "é€ä¿¡å…ˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å…¥åŠ›ã—ã¦ä¸‹ã•ã„",
            'app_password': "Googleã®ã‚¢ãƒ—ãƒªãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã§ã™",
            
            'min_word_length': 2,
            'min_frequency': 3,
            'network_top_n': 40,
            'topic_num': 5,
            'cluster_num': 7,
            
            # æ–°ã—ã„è¨­å®šé …ç›®ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
            'enable_verb_normalization': True,      # å‹•è©ã®åŸå½¢åŒ–ã‚’æœ‰åŠ¹
            'strict_pos_filtering': True,           # å³å¯†ãªå“è©ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹
            'exclude_inference_emotion': True,      # æ¨è«–ãƒ»æ„Ÿæƒ…èªã®é™¤å¤–ã‚’æœ‰åŠ¹
            'exclude_structural_words': True,       # æ§‹é€ èªã®é™¤å¤–ã‚’æœ‰åŠ¹
            'min_word_importance': 0.01,           # èªå½™é‡è¦åº¦ã®æœ€å°é–¾å€¤
            'enable_semantic_filtering': True,      # æ„å‘³çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹
        }
    
    def _load_config(self, path):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def enhanced_tokenize(self, text):
        """è¨€èªå­¦çš„çŸ¥è¦‹ã«åŸºã¥ãé«˜åº¦ãªå½¢æ…‹ç´ è§£æï¼ˆæ”¹è‰¯ç‰ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰"""
        words = []
        
        if self.use_mecab:
            # MeCabã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦è§£æ
            node = self.mecab.parseToNode(text)
            while node:
                if node.surface:
                    surface = node.surface
                    features = node.feature.split(',')
                    
                    if len(features) >= 4:
                        pos_major = features[0]
                        pos_minor1 = features[1]
                        pos_minor2 = features[2]
                        base_form = features[6] if len(features) > 6 and features[6] != '*' else surface
                        
                        # æ”¹è‰¯ç‰ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if self._is_meaningful_word_enhanced(surface, base_form, pos_major, pos_minor1, pos_minor2):
                            # å‹•è©ã¯åŸå½¢ã‚’ä½¿ç”¨ã€ãã®ä»–ã¯è¡¨å±¤å½¢ã‚’ä½¿ç”¨
                            word = base_form if pos_major == 'å‹•è©' and self.config['enable_verb_normalization'] else surface
                            words.append(word)
                
                node = node.next
        else:
            # Janomeã«ã‚ˆã‚‹è§£æï¼ˆæ”¹è‰¯ç‰ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
            tokens = self.tokenizer.tokenize(text)
            for token in tokens:
                surface = token.surface
                pos_info = token.part_of_speech.split(',')
                
                if len(pos_info) >= 2:
                    pos_major = pos_info[0]
                    pos_minor1 = pos_info[1] if len(pos_info) > 1 else ''
                    pos_minor2 = pos_info[2] if len(pos_info) > 2 else ''
                    base_form = pos_info[6] if len(pos_info) > 6 and pos_info[6] != '*' else surface
                    
                    if self._is_meaningful_word_enhanced(surface, base_form, pos_major, pos_minor1, pos_minor2):
                        word = base_form if pos_major == 'å‹•è©' and self.config['enable_verb_normalization'] else surface
                        words.append(word)
        
        return words
    
    def _is_meaningful_word_enhanced(self, surface, base_form, pos_major, pos_minor1, pos_minor2):
        """æ”¹è‰¯ç‰ˆï¼šèªå½™ãŒåˆ†æå¯¾è±¡ã¨ã—ã¦æ„å‘³ãŒã‚ã‚‹ã‹ã‚’åˆ¤å®šã™ã‚‹é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿"""
        
        # åŸºæœ¬çš„ãªé™¤å¤–æ¡ä»¶
        if (len(surface) < self.config['min_word_length'] or 
            surface.isascii() or 
            surface.isdigit()):
            return False
        
        # æ”¹è‰¯ç‰ˆï¼šã‚«ãƒ†ã‚´ãƒªåˆ¥é™¤å¤–ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šç¢ºå®Ÿãªæ–¹æ³•ï¼‰
        if self.config.get('exclude_inference_emotion', True):
            if surface in self.inference_emotion_words or base_form in self.inference_emotion_words:
                return False
        
        if self.config.get('exclude_structural_words', True):
            if surface in self.structural_words or base_form in self.structural_words:
                return False
        
        # æ©Ÿèƒ½èªã®ãƒã‚§ãƒƒã‚¯
        if surface in self.functional_words or base_form in self.functional_words:
            return False
            
        # æ•¬ç§°ãƒ»æ•¬èªã®ãƒã‚§ãƒƒã‚¯ï¼ˆæ–°è¨­ï¼‰
        if surface in self.honorific_words or base_form in self.honorific_words:
            return False
        
        # ã•ã‚‰ã«è©³ç´°ãªé™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ•¬ç§°ã¨æ§˜æ…‹è¡¨ç¾ã‚’å¼·åŒ–ï¼‰
        additional_patterns = {
            # æ•¬ç§°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç¢ºå®Ÿã«é™¤å¤–ï¼‰
            'honorifics_strict': {'ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“', 'æ§˜', 'ã•ã¾', 'æ°', 'å›'},
            
            # æ§˜æ…‹è¡¨ç¾ï¼ˆç¢ºå®Ÿã«é™¤å¤–ï¼‰
            'modal_expressions': {
                'ã‚ˆã†', 'ã‚ˆã†ãª', 'ã‚ˆã†ã«', 'ã‚ˆã†ã ', 'ã‚ˆã†ã§', 'ã‚ˆã†ã§ã™',
                'ã¿ãŸã„', 'ã¿ãŸã„ãª', 'ã¿ãŸã„ã«', 'ã¿ãŸã„ã ', 'ã¿ãŸã„ã§',
                'ã£ã½ã„', 'ã£ã½ã', 'ã£ã½ã•', 'ã‚‰ã—ã„', 'ã‚‰ã—ã', 'ã‚‰ã—ã•'
            },
            
            # äººåãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ‹¡å¼µï¼‰
            'person_names': {'ã‚¸ãƒ’ãƒ§', 'ãƒã‚§ãƒ¨ãƒ³', 'ãƒ„ã‚¦ã‚£', 'ãƒŠãƒ¨ãƒ³', 'ãƒ¢ãƒ¢', 'ã‚µãƒŠ', 'ãƒ€ãƒ’ãƒ§ãƒ³', 'ã‚¸ãƒ§ãƒ³ãƒ¨ãƒ³', 'ãƒŸãƒŠ'},
            
            # è¨˜å·çš„è¡¨ç¾
            'symbols': {'ã€‚', 'ã€', 'ï¼', 'ï¼Ÿ', ')', '(', 'ã€', 'ã€Œ', 'ã€', 'ã€', 'ã€', 'ã€‘', 'ã€ˆ', 'ã€‰'},
            
            # å˜ä½ãƒ»åŠ©æ•°è©
            'units': {'å††', 'ä¸‡', 'åƒ', 'ç™¾', 'å„„', 'å…†', 'kg', 'km', 'cm', 'mm', 'g', 'ml', 'l'},
            
            # ä¸€èˆ¬çš„ã™ãã‚‹å‰¯è©
            'common_adverbs': {'ã¨ã¦ã‚‚', 'ã‹ãªã‚Š', 'ãšã„ã¶ã‚“', 'ã ã„ã¶', 'ã‚ã‚Šã¨', 'ã‘ã£ã“ã†', 'ã¡ã‚‡ã£ã¨', 'ã™ã“ã—', 'å°‘ã—'}
        }
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°é™¤å¤–ï¼ˆç¢ºå®Ÿæ€§ã‚’å‘ä¸Šï¼‰
        for pattern_type, pattern_set in additional_patterns.items():
            if surface in pattern_set or base_form in pattern_set:
                return False
        
        # èªå°¾ã«ã‚ˆã‚‹æ•¬ç§°ãƒã‚§ãƒƒã‚¯ï¼ˆã•ã‚‰ãªã‚‹ç¢ºå®Ÿæ€§ã®ãŸã‚ï¼‰
        honorific_suffixes = ['ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“', 'æ§˜', 'ã•ã¾', 'æ°']
        for suffix in honorific_suffixes:
            if surface.endswith(suffix) or base_form.endswith(suffix):
                return False
        
        # æ§˜æ…‹è¡¨ç¾ã®èªå°¾ãƒã‚§ãƒƒã‚¯
        modal_suffixes = ['ã‚ˆã†', 'ã‚ˆã†ãª', 'ã‚ˆã†ã«', 'ã¿ãŸã„', 'ã‚‰ã—ã„', 'ã£ã½ã„']
        for suffix in modal_suffixes:
            if surface.endswith(suffix) or base_form.endswith(suffix):
                return False
        
        # å“è©ã«ã‚ˆã‚‹è©³ç´°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if pos_major == 'åè©':
            if pos_minor1 in ['ä¸€èˆ¬', 'å›ºæœ‰åè©', 'ã‚µå¤‰æ¥ç¶š']:
                # å›ºæœ‰åè©ã®å ´åˆã€ã‚ˆã‚Šå³å¯†ãªãƒã‚§ãƒƒã‚¯
                if pos_minor1 == 'å›ºæœ‰åè©':
                    # äººåã‚‰ã—ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
                    if (len(surface) <= 4 and 
                        all(ord(char) >= 0x30A0 and ord(char) <= 0x30FF for char in surface)):
                        return False
                    # çµ„ç¹”åã®ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
                    org_suffixes = {'ä¼šç¤¾', 'æ ªå¼ä¼šç¤¾', 'æœ‰é™ä¼šç¤¾', 'è²¡å›£æ³•äºº', 'ç¤¾å›£æ³•äºº', 'å¤§å­¦', 'å­¦æ ¡', 'ç—…é™¢'}
                    if any(surface.endswith(suffix) for suffix in org_suffixes):
                        return False
                return True
            elif pos_minor1 in ['ä»£åè©', 'æ•°']:
                return False
            elif pos_minor2 in ['åŠ©æ•°è©', 'æ¥å°¾', 'éè‡ªç«‹']:
                return False
            else:
                return len(surface) >= 2  # çŸ­ã™ãã‚‹åè©ã‚’é™¤å¤–
                
        elif pos_major == 'å‹•è©':
            if pos_minor1 in ['è‡ªç«‹']:
                # ä¸€èˆ¬çš„ã™ãã‚‹å‹•è©ã®æ‹¡å¼µãƒªã‚¹ãƒˆ
                return base_form not in self.functional_words
            else:
                return False
                
        elif pos_major == 'å½¢å®¹è©':
            if pos_minor1 in ['è‡ªç«‹']:
                # ä¸€èˆ¬çš„ã™ãã‚‹å½¢å®¹è©ã‚’é™¤å¤–
                return base_form not in {'è‰¯ã„', 'ã‚ˆã„', 'ã„ã„', 'æ‚ªã„', 'ã‚ã‚‹ã„', 'å¤šã„', 'å°‘ãªã„', 'å¤§ãã„', 'å°ã•ã„'}
            else:
                return False
        
        elif pos_major == 'å‰¯è©':
            # å‰¯è©ã¯æ§‹é€ èªã«å¤šãå«ã¾ã‚Œã‚‹ãŸã‚ã€ã‚ˆã‚Šå³ã—ããƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.config.get('strict_pos_filtering', True):
                return False  # å‰¯è©ã¯åŸºæœ¬çš„ã«é™¤å¤–
            else:
                return surface not in additional_patterns['common_adverbs']
        
        # ãã®ä»–ã®å“è©ã¯é™¤å¤–
        return False
    
    def extract_enhanced_features(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        words = self.enhanced_tokenize(text)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã®å‡ºåŠ›
        if self.config.get('enable_semantic_filtering', True):
            print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®èªå½™æ•°: {len(words)}èª")
        
        # åŸºæœ¬çµ±è¨ˆ
        char_count = len(text)
        word_count = len(words)
        unique_words = len(set(words))
        avg_word_length = np.mean([len(w) for w in words]) if words else 0
        
        # èªå½™ã®è±Šå¯Œã•ï¼ˆTTR: Type-Token Ratioï¼‰
        ttr = unique_words / word_count if word_count > 0 else 0
        
        # å…±èµ·ãƒšã‚¢ã®æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        weighted_pairs = Counter()
        
        # å‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã§ã®å…±èµ·æŠ½å‡º
        window_sizes = [3, 5, 10]
        
        for window in window_sizes:
            weight = 1.0 / window
            for i in range(len(words)):
                for j in range(i + 1, min(i + window, len(words))):
                    pair = tuple(sorted((words[i], words[j])))
                    weighted_pairs[pair] += weight
        
        return {
            'words': words,
            'pairs': weighted_pairs,
            'word_count': word_count,
            'unique_words': unique_words,
            'char_count': char_count,
            'avg_word_length': avg_word_length,
            'ttr': ttr,
            'word_frequency': Counter(words)
        }
    
    def create_filtering_report(self):
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = f"""
ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šãƒ¬ãƒãƒ¼ãƒˆã€‘

â–  é™¤å¤–èªã‚«ãƒ†ã‚´ãƒªè¨­å®š
ãƒ»æ¨è«–ãƒ»æ„Ÿæƒ…èªé™¤å¤–: {'æœ‰åŠ¹' if self.config.get('exclude_inference_emotion', True) else 'ç„¡åŠ¹'}
ãƒ»æ§‹é€ èªé™¤å¤–: {'æœ‰åŠ¹' if self.config.get('exclude_structural_words', True) else 'ç„¡åŠ¹'}
ãƒ»å³å¯†ãªå“è©ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {'æœ‰åŠ¹' if self.config.get('strict_pos_filtering', True) else 'ç„¡åŠ¹'}

â–  é™¤å¤–èªçµ±è¨ˆ
ãƒ»æ¨è«–ãƒ»æ„Ÿæƒ…èª: {len(self.inference_emotion_words)}èª
  ä¾‹: {', '.join(list(self.inference_emotion_words)[:10])}...

ãƒ»æ§‹é€ èª: {len(self.structural_words)}èª
  ä¾‹: {', '.join(list(self.structural_words)[:10])}...

ãƒ»æ©Ÿèƒ½èª: {len(self.functional_words)}èª
  ä¾‹: {', '.join(list(self.functional_words)[:10])}...

ãƒ»æ•¬ç§°ãƒ»æ•¬èª: {len(self.honorific_words)}èª
  ä¾‹: {', '.join(list(self.honorific_words)[:10])}...

ãƒ»ç·é™¤å¤–èªæ•°: {len(self.all_excluded_words)}èª

â–  ç‰¹åˆ¥å¼·åŒ–ã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
ãƒ»æ•¬ç§°ã®å®Œå…¨é™¤å»: ã€Œã•ã‚“ã€ã€Œã¡ã‚ƒã‚“ã€ã€Œãã‚“ã€ã€Œæ§˜ã€ç­‰ã®ç¢ºå®Ÿãªé™¤å¤–
ãƒ»æ§˜æ…‹è¡¨ç¾ã®é™¤å»: ã€Œã‚ˆã†ã€ã€Œã‚ˆã†ãªã€ã€Œã¿ãŸã„ã€ã€Œã‚‰ã—ã„ã€ç­‰ã®åŒ…æ‹¬çš„é™¤å¤–
ãƒ»èªå°¾ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°: ã‚ˆã‚Šç¢ºå®Ÿãªé™¤å¤–ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

â–  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ
ã“ã®è¨­å®šã«ã‚ˆã‚Šã€åˆ†æã®ç„¦ç‚¹ã¯ä»¥ä¸‹ã®èªå½™ã‚¿ã‚¤ãƒ—ã«çµã‚‰ã‚Œã¾ã™ï¼š
ãƒ»å®Ÿè³ªçš„ãªæ„å‘³ã‚’æŒã¤åè©ï¼ˆä¸€èˆ¬åè©ã€å°‚é–€ç”¨èªï¼‰
ãƒ»é‡è¦ãªå‹•ä½œã‚’è¡¨ã™å‹•è©
ãƒ»ç‰¹å¾´çš„ãªå½¢å®¹è©
ãƒ»å›ºæœ‰åè©ï¼ˆäººåãƒ»çµ„ç¹”åãƒ»æ•¬ç§°ã¯é™¤å¤–ï¼‰

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã®æ ¸å¿ƒçš„ãªå†…å®¹ãŒã‚ˆã‚Šæ˜ç¢ºã«æµ®ã‹ã³ä¸ŠãŒã‚Šã¾ã™ã€‚
        """
        return report.strip()
    
    def create_interactive_network(self, pair_counter, output_path):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ä½œæˆ"""
        # NetworkXã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
        G = nx.Graph()
        top_pairs = pair_counter.most_common(self.config['network_top_n'])
        
        for (w1, w2), weight in top_pairs:
            G.add_edge(w1, w2, weight=weight)
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®è¨ˆç®—
        layouts = {
            'spring': nx.spring_layout(G, k=3, iterations=50),
            'kamada_kawai': nx.kamada_kawai_layout(G),
            'circular': nx.circular_layout(G)
        }
        
        # æœ€ã‚‚åˆ†æ•£ã®å¤§ãã„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’é¸æŠ
        best_layout = max(layouts.items(), 
                         key=lambda x: np.var([pos[0] for pos in x[1].values()]))
        pos = best_layout[1]
        
        # ãƒãƒ¼ãƒ‰ã®é‡è¦åº¦è¨ˆç®—
        centrality = nx.degree_centrality(G)
        betweenness = nx.betweenness_centrality(G)
        pagerank = nx.pagerank(G)
        
        # Plotlyã§ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–
        edge_x, edge_y = [], []
        edge_info = []
        
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            
            weight = G[edge[0]][edge[1]]['weight']
            edge_info.append(f"{edge[0]} - {edge[1]}: {weight:.3f}")
        
        # ã‚¨ãƒƒã‚¸ã®æç”»
        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=0.5, color='#888'),
            hoverinfo='none',
            mode='lines'
        )
        
        # ãƒãƒ¼ãƒ‰ã®æº–å‚™
        node_x = [pos[node][0] for node in G.nodes()]
        node_y = [pos[node][1] for node in G.nodes()]
        
        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            hoverinfo='text',
            text=[node for node in G.nodes()],
            textposition="middle center",
            marker=dict(
                showscale=True,
                colorscale='Viridis',
                reversescale=True,
                color=[],
                size=[],
                colorbar=dict(
                    thickness=15,
                    len=0.5,
                    x=1.02,
                    title="é‡è¦åº¦"
                ),
                line=dict(width=2)
            )
        )
        
        # ãƒãƒ¼ãƒ‰ã®è‰²ã¨ã‚µã‚¤ã‚ºã‚’é‡è¦åº¦ã«åŸºã¥ã„ã¦è¨­å®š
        node_adjacencies = []
        node_sizes = []
        node_text = []
        
        for node in G.nodes():
            adjacencies = list(G.neighbors(node))
            node_adjacencies.append(len(adjacencies))
            
            # ã‚µã‚¤ã‚ºã¯æ¬¡æ•°ã¨PageRankã®çµ„ã¿åˆã‚ã›
            size = 20 + (centrality[node] * 40) + (pagerank[node] * 100)
            node_sizes.append(size)
            
            # ãƒ›ãƒãƒ¼æƒ…å ±
            info = (f"èª: {node}<br>"
                   f"æ¥ç¶šæ•°: {len(adjacencies)}<br>"
                   f"æ¬¡æ•°ä¸­å¿ƒæ€§: {centrality[node]:.3f}<br>"
                   f"åª’ä»‹ä¸­å¿ƒæ€§: {betweenness[node]:.3f}<br>"
                   f"PageRank: {pagerank[node]:.3f}")
            node_text.append(info)
        
        node_trace.marker.color = node_adjacencies
        node_trace.marker.size = node_sizes
        node_trace.hovertext = node_text
        
        # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
        fig = go.Figure(data=[edge_trace, node_trace],
                       layout=go.Layout(
                           title={
                               'text': 'èªå½™å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼šé«˜ç²¾åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç‰ˆ',
                               'x': 0.5,
                               'font': {'size': 20}
                           },
                           showlegend=False,
                           hovermode='closest',
                           margin=dict(b=20,l=5,r=5,t=40),
                           annotations=[ dict(
                               text="æ¨è«–ãƒ»æ„Ÿæƒ…èªãƒ»æ§‹é€ èªã‚’é™¤å¤–ã—ãŸã€æ ¸å¿ƒçš„ãªèªå½™é–¢ä¿‚ã‚’è¡¨ç¤º",
                               showarrow=False,
                               xref="paper", yref="paper",
                               x=0.005, y=-0.002,
                               xanchor='left', yanchor='bottom',
                               font=dict(size=12)
                           )],
                           xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                           plot_bgcolor='white'
                       ))
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        html_path = output_path.replace('.png', '_interactive.html')
        fig.write_html(html_path)
        
        # é™çš„ãªç”»åƒã‚‚ç”Ÿæˆ
        self._create_static_network(pair_counter, output_path)
        
        return html_path
    
    def _create_static_network(self, pair_counter, output_path):
        """é™çš„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ç”Ÿæˆ"""
        G = nx.Graph()
        top_pairs = pair_counter.most_common(self.config['network_top_n'])
        
        for (w1, w2), weight in top_pairs:
            G.add_edge(w1, w2, weight=weight)
        
        if not G.edges():
            print("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã‚’ä½œæˆã™ã‚‹ã®ã«ååˆ†ãªå…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        # ç¾ä»£ã®matplotlibå¯¾å¿œ
        fig, ax = plt.subplots(figsize=(16, 12))
        pos = nx.kamada_kawai_layout(G)
        
        # ãƒãƒ¼ãƒ‰ã®é‡è¦åº¦è¨ˆç®—
        centrality = nx.degree_centrality(G)
        
        # ã‚¨ãƒƒã‚¸ã®é‡ã¿æ­£è¦åŒ–
        weights = [G[u][v]['weight'] for u, v in G.edges()]
        norm = Normalize(vmin=min(weights), vmax=max(weights))
        
        cmap = matplotlib.colormaps['Spectral']
        
        # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºè¨ˆç®—
        node_sizes = [500 + 1000 * centrality[node] for node in G.nodes()]
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã§èª­ã¿ã‚„ã™ã„æç”»è¨­å®š
        nx.draw_networkx_nodes(
            G, pos, 
            node_color='lightblue',
            node_size=node_sizes,
            alpha=0.8,
            edgecolors='lightgray',
            linewidths=0.5,
            ax=ax
        )
        
        nx.draw_networkx_edges(
            G, pos,
            width=[1 + 3 * norm(w) for w in weights],
            edge_color=[cmap(norm(w)) for w in weights],
            alpha=0.7,
            ax=ax
        )
        
        # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¦èª­ã¿ã‚„ã™ã•ã‚’å‘ä¸Š
        nx.draw_networkx_labels(
            G, pos,
            font_size=12,
            font_family='IPAGothic',
            font_weight='bold',
            ax=ax
        )
        
        ax.axis('off')
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_wordcloud(self, word_freq, output_path):
        """æ—¥æœ¬èªå¯¾å¿œã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ"""
        # é »åº¦è¾æ›¸ã®æº–å‚™
        filtered_freq = {word: freq for word, freq in word_freq.items() 
                        if freq >= self.config['min_frequency']}
        
        if not filtered_freq:
            return None
            
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆ
        wordcloud = WordCloud(
            font_path='/usr/share/fonts/truetype/fonts-japanese-gothic.ttf',
            width=1200, height=800,
            background_color='white',
            max_words=100,
            colormap='viridis',
            relative_scaling=0.5,
            min_font_size=10
        ).generate_from_frequencies(filtered_freq)
        
        plt.figure(figsize=(12, 8))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return output_path
    
    def advanced_topic_modeling(self, texts):
        """æ”¹è‰¯ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°"""
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        processed_docs = []
        for text in texts:
            words = self.enhanced_tokenize(text)
            processed_docs.append(' '.join(words))
        
        if not any(processed_docs):
            return None, None
            
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        vectorizer = TfidfVectorizer(
            max_features=1000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(processed_docs)
            feature_names = vectorizer.get_feature_names_out()
            
            # æœ€é©ãªãƒˆãƒ”ãƒƒã‚¯æ•°ã®æ±ºå®š
            best_topics = self._find_optimal_topics(processed_docs)
            
            # LDAãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
            lda = LatentDirichletAllocation(
                n_components=best_topics,
                random_state=42,
                max_iter=100
            )
            
            lda.fit(tfidf_matrix)
            
            # ãƒˆãƒ”ãƒƒã‚¯ã®æŠ½å‡º
            topics = []
            for topic_idx, topic in enumerate(lda.components_):
                top_words_idx = topic.argsort()[-10:][::-1]
                top_words = [feature_names[i] for i in top_words_idx]
                topic_weight = topic[top_words_idx]
                
                topic_info = {
                    'id': topic_idx,
                    'words': list(zip(top_words, topic_weight)),
                    'description': ' + '.join([f"{word}({weight:.3f})" 
                                             for word, weight in zip(top_words[:5], topic_weight[:5])])
                }
                topics.append(topic_info)
            
            return topics, lda
            
        except Exception as e:
            print(f"ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    def _find_optimal_topics(self, docs, max_topics=10):
        """æœ€é©ãªãƒˆãƒ”ãƒƒã‚¯æ•°ã‚’è¦‹ã¤ã‘ã‚‹"""
        try:
            from gensim.corpora import Dictionary
            from gensim.models import LdaModel
            from gensim.models.coherencemodel import CoherenceModel
            
            # Gensimã§ã®ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹è¨ˆç®—
            texts = [doc.split() for doc in docs]
            dictionary = Dictionary(texts)
            corpus = [dictionary.doc2bow(text) for text in texts]
            
            coherence_scores = []
            for num_topics in range(2, min(max_topics + 1, len(docs))):
                lda_model = LdaModel(
                    corpus=corpus,
                    id2word=dictionary,
                    num_topics=num_topics,
                    random_state=42,
                    passes=10
                )
                
                coherence_model = CoherenceModel(
                    model=lda_model,
                    texts=texts,
                    dictionary=dictionary,
                    coherence='c_v'
                )
                
                coherence_scores.append(coherence_model.get_coherence())
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒˆãƒ”ãƒƒã‚¯æ•°ã‚’è¿”ã™
            best_num = range(2, min(max_topics + 1, len(docs)))[np.argmax(coherence_scores)]
            return best_num
            
        except:
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            return min(self.config['topic_num'], len(docs) // 2)
    
    def create_analysis_dashboard(self, all_features, topics, output_dir):
        """åˆ†æçµæœã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ"""
        # è¤‡æ•°ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’å«ã‚€ç·åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('èªå½™é »åº¦åˆ†å¸ƒ', 'ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ', 'æ–‡æ›¸çµ±è¨ˆ', 'ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ'),
            specs=[[{"type": "bar"}, {"type": "pie"}],
                   [{"type": "scatter"}, {"type": "bar"}]]
        )
        
        # 1. èªå½™é »åº¦åˆ†å¸ƒ
        word_freq = Counter()
        for features in all_features:
            word_freq.update(features['word_frequency'])
        
        top_words = word_freq.most_common(20)
        if top_words:
            fig.add_trace(
                go.Bar(x=[w[0] for w in top_words], y=[w[1] for w in top_words],
                       name="èªå½™é »åº¦", marker_color='skyblue'),
                row=1, col=1
            )
        
        # 2. ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰
        if topics:
            topic_labels = [f"ãƒˆãƒ”ãƒƒã‚¯{t['id']+1}" for t in topics]
            topic_values = [len(t['words']) for t in topics]
            
            fig.add_trace(
                go.Pie(labels=topic_labels, values=topic_values, name="ãƒˆãƒ”ãƒƒã‚¯"),
                row=1, col=2
            )
        
        # 3. æ–‡æ›¸çµ±è¨ˆï¼ˆæ•£å¸ƒå›³ï¼‰
        doc_stats = pd.DataFrame([
            {
                'TTR': f['ttr'],
                'èªæ•°': f['word_count'],
                'æ–‡å­—æ•°': f['char_count'],
                'å¹³å‡èªé•·': f['avg_word_length']
            }
            for f in all_features
        ])
        
        if not doc_stats.empty:
            fig.add_trace(
                go.Scatter(
                    x=doc_stats['èªæ•°'], 
                    y=doc_stats['TTR'],
                    mode='markers',
                    marker=dict(
                        size=doc_stats['æ–‡å­—æ•°']/100,
                        color=doc_stats['å¹³å‡èªé•·'],
                        colorscale='Viridis',
                        showscale=True
                    ),
                    name="æ–‡æ›¸ç‰¹æ€§"
                ),
                row=2, col=1
            )
        
        # 4. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ
        filter_categories = ['æ¨è«–ãƒ»æ„Ÿæƒ…èª', 'æ§‹é€ èª', 'æ©Ÿèƒ½èª', 'æ•¬ç§°ãƒ»æ•¬èª']
        filter_counts = [
            len(self.inference_emotion_words),
            len(self.structural_words), 
            len(self.functional_words),
            len(self.honorific_words)  # æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒª
        ]
        
        fig.add_trace(
            go.Bar(x=filter_categories, y=filter_counts,
                   name="é™¤å¤–èªæ•°", marker_color='coral'),
            row=2, col=2
        )
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
        fig.update_layout(
            height=800,
            title_text="ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ç·åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆæ”¹è‰¯ç‰ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰",
            title_x=0.5,
            showlegend=False
        )
        
        # HTMLã¨ã—ã¦ä¿å­˜
        dashboard_path = os.path.join(output_dir, 'analysis_dashboard.html')
        fig.write_html(dashboard_path)
        
        return dashboard_path
    
    def generate_comprehensive_report(self, all_features, topics, pair_counter):
        """åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        # å…¨ä½“çµ±è¨ˆã®è¨ˆç®—
        total_words = sum(f['word_count'] for f in all_features)
        total_chars = sum(f['char_count'] for f in all_features)
        unique_words = len(set().union(*[f['words'] for f in all_features]))
        
        avg_ttr = np.mean([f['ttr'] for f in all_features])
        avg_word_length = np.mean([f['avg_word_length'] for f in all_features])
        
        # æœ€é »å‡ºèªã®åˆ†æ
        all_word_freq = Counter()
        for features in all_features:
            all_word_freq.update(features['word_frequency'])
        
        top_words = all_word_freq.most_common(15)
        
        # å…±èµ·é–¢ä¿‚ã®åˆ†æ
        top_cooccurrences = pair_counter.most_common(15)
        
        # ãƒˆãƒ”ãƒƒã‚¯è¦ç´„
        topic_summary = ""
        if topics:
            for i, topic in enumerate(topics):
                top_5_words = [word for word, _ in topic['words'][:5]]
                topic_summary += f"ãƒ»ãƒˆãƒ”ãƒƒã‚¯{i+1}: {' / '.join(top_5_words)}\n"
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœã®çµ±è¨ˆ
        filtering_stats = self.create_filtering_report()
        
        # ãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡ã®ç”Ÿæˆ
        report = f"""
ã€é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‘
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

{filtering_stats}

â–  åŸºæœ¬çµ±è¨ˆæƒ…å ±
ãƒ»åˆ†æå¯¾è±¡æ–‡æ›¸æ•°: {len(all_features)}ä»¶
ãƒ»ç·èªæ•°: {total_words:,}èªï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰
ãƒ»ç·æ–‡å­—æ•°: {total_chars:,}æ–‡å­—
ãƒ»ãƒ¦ãƒ‹ãƒ¼ã‚¯èªæ•°: {unique_words:,}èª
ãƒ»å¹³å‡èªå½™è±Šå¯Œåº¦(TTR): {avg_ttr:.3f}
ãƒ»å¹³å‡èªé•·: {avg_word_length:.2f}æ–‡å­—

â–  é‡è¦èªãƒˆãƒƒãƒ—15ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨å¾Œï¼‰
{chr(10).join([f'ãƒ»{word}: {freq}å›' for word, freq in top_words])}

â–  æ³¨ç›®ã•ã‚Œã‚‹å…±èµ·é–¢ä¿‚ãƒˆãƒƒãƒ—15
{chr(10).join([f'ãƒ»ã€Œ{w1}ã€ã¨ã€Œ{w2}ã€: é–¢é€£åº¦{freq:.3f}' for (w1, w2), freq in top_cooccurrences])}

â–  ç™ºè¦‹ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯
{topic_summary}

â–  åˆ†æã®æ´å¯Ÿï¼ˆæ”¹è‰¯ç‰ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨ï¼‰
ã“ã®é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ãªç‰¹å¾´ãŒæ˜ç¢ºã«ãªã‚Šã¾ã—ãŸï¼š

1. èªå½™ã®è³ªçš„åˆ†æ: æ¨è«–ãƒ»æ„Ÿæƒ…èªã¨æ§‹é€ èªã‚’é™¤å¤–ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã®æ ¸å¿ƒçš„ãªå†…å®¹èªã«ç„¦ç‚¹ãŒå½“ãŸã‚Šã¾ã—ãŸã€‚

2. æ„å‘³çš„ä¸­å¿ƒæ€§: æœ€é »å‡ºèªã€Œ{top_words[0][0] if top_words else "N/A"}ã€ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®å®Ÿè³ªçš„ãªãƒ†ãƒ¼ãƒã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚

3. æ¦‚å¿µçš„é–¢é€£æ€§: å…±èµ·åˆ†æã«ã‚ˆã‚Šã€æ„å‘³çš„ã«é‡è¦ãªèªå½™é–“ã®é–¢ä¿‚ãŒæµ®ãå½«ã‚Šã«ãªã‚Šã¾ã—ãŸã€‚

4. ãƒ†ãƒ¼ãƒæ§‹é€ : {len(topics) if topics else 0}å€‹ã®æ˜ç¢ºãªãƒˆãƒ”ãƒƒã‚¯ãŒç‰¹å®šã•ã‚Œã€å†…å®¹ã®æ§‹é€ åŒ–ã•ã‚ŒãŸç†è§£ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚

â–  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®åŠ¹æœ
ãƒ»é™¤å»ã•ã‚ŒãŸèªå½™ã‚«ãƒ†ã‚´ãƒªã«ã‚ˆã‚Šã€åˆ†æã®ç²¾åº¦ãŒå¤§å¹…ã«å‘ä¸Š
ãƒ»æ¨è«–è¡¨ç¾ã‚„æ§‹é€ èªã®é™¤å»ã«ã‚ˆã‚Šã€å®Ÿè³ªçš„ãªå†…å®¹ã«é›†ä¸­
ãƒ»æ„Ÿæƒ…è¡¨ç¾ã®é™¤å»ã«ã‚ˆã‚Šã€å®¢è¦³çš„ãªå†…å®¹åˆ†æãŒå®Ÿç¾

â–  æŠ€è¡“çš„è©³ç´°
ãƒ»æ”¹è‰¯ç‰ˆå½¢æ…‹ç´ è§£æ: Janome/MeCab + è¨€èªå­¦çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
ãƒ»é™¤å¤–èªè¾æ›¸: æ¨è«–ãƒ»æ„Ÿæƒ…èªã€æ§‹é€ èªã€æ©Ÿèƒ½èªã®ä½“ç³»çš„åˆ†é¡
ãƒ»å¯è¦–åŒ–: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€æ”¹è‰¯ç‰ˆãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
ãƒ»åˆ†ææ‰‹æ³•: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€LDAãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€TF-IDF

â€» ã“ã®åˆ†æã¯é«˜åº¦ãªè‡ªç„¶è¨€èªå‡¦ç†æŠ€è¡“ã¨è¨€èªå­¦çš„çŸ¥è¦‹ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚
â€» ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã®æœ¬è³ªçš„ãªå†…å®¹æ§‹é€ ãŒæ˜ç¢ºåŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚
        """
        
        return report.strip()
    
    def process_files(self):
        """ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†å®Ÿè¡Œï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        print("=== é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æé–‹å§‹ ===")
        print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š: æ¨è«–ãƒ»æ„Ÿæƒ…èªé™¤å¤–={self.config.get('exclude_inference_emotion', True)}")
        print(f"                    æ§‹é€ èªé™¤å¤–={self.config.get('exclude_structural_words', True)}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
        os.makedirs(self.config['archive_dir'], exist_ok=True)
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        all_features = []
        all_pair_counter = Counter()
        all_texts = []
        
        source_files = [f for f in os.listdir(self.config['source_dir']) 
                       if f.endswith('.txt')]
        
        if not source_files:
            print("å‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        print(f"{len(source_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
        
        for file in source_files:
            file_path = os.path.join(self.config['source_dir'], file)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                if text.strip():
                    features = self.extract_enhanced_features(text)
                    all_features.append(features)
                    all_pair_counter.update(features['pairs'])
                    all_texts.append(text)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ç§»å‹•
                shutil.move(file_path, os.path.join(self.config['archive_dir'], file))
                
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«{file}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not all_features:
            print("å‡¦ç†å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        print("é«˜åº¦åˆ†æã‚’å®Ÿè¡Œä¸­...")
        
        # å„ç¨®åˆ†æã®å®Ÿè¡Œ
        try:
            # 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã¨ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
            network_path = os.path.join(self.config['output_dir'], 'network_filtered.png')
            interactive_network = self.create_interactive_network(all_pair_counter, network_path)
            
            # 2. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
            all_word_freq = Counter()
            for features in all_features:
                all_word_freq.update(features['word_frequency'])
            
            wordcloud_path = os.path.join(self.config['output_dir'], 'wordcloud_filtered.png')
            self.create_wordcloud(all_word_freq, wordcloud_path)
            
            # 3. ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
            topics, lda_model = self.advanced_topic_modeling(all_texts)
            
            # 4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
            dashboard_path = self.create_analysis_dashboard(all_features, topics, self.config['output_dir'])
            
            # 5. åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = self.generate_comprehensive_report(all_features, topics, all_pair_counter)
            
            # çµæœã®ä¿å­˜
            self.results = {
                'report': report,
                'network_path': network_path,
                'interactive_network': interactive_network,
                'wordcloud_path': wordcloud_path,
                'dashboard_path': dashboard_path,
                'topics': topics
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
            report_path = os.path.join(self.config['output_dir'], 'analysis_report_filtered.txt')
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            print("=== åˆ†æå®Œäº†ï¼ ===")
            print(f"ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³: {network_path}")
            print(f"ãƒ»ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ç‰ˆ: {interactive_network}")
            print(f"ãƒ»ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰: {wordcloud_path}")
            print(f"ãƒ»ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: {dashboard_path}")
            print(f"ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœã®è¡¨ç¤º
            total_excluded = len(self.all_excluded_words)
            print(f"\nğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆ:")
            print(f"ãƒ»ç·é™¤å¤–èªæ•°: {total_excluded}èª")
            print(f"ãƒ»æ¨è«–ãƒ»æ„Ÿæƒ…èª: {len(self.inference_emotion_words)}èª")
            print(f"ãƒ»æ§‹é€ èª: {len(self.structural_words)}èª")
            print(f"ãƒ»æ©Ÿèƒ½èª: {len(self.functional_words)}èª")
            print(f"ãƒ»æ•¬ç§°ãƒ»æ•¬èª: {len(self.honorific_words)}èª")
            print(f"\nâœ… ç‰¹åˆ¥å¼·åŒ–: ã€Œã•ã‚“ã€ã€Œã‚ˆã†ã€ç­‰ã®ç¢ºå®Ÿãªé™¤å¤–ã‚’å®Ÿè£…")
            
            # ãƒ¡ãƒ¼ãƒ«é€ä¿¡
            self.send_enhanced_email()
            
        except Exception as e:
            print(f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            raise
    
    def send_enhanced_email(self):
        """æ”¹è‰¯ã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«é€ä¿¡æ©Ÿèƒ½"""
        if not self.results:
            print("é€ä¿¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        try:
            msg = MIMEMultipart()
            msg['Subject'] = "é«˜åº¦ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ”¹è‰¯ç‰ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰"
            msg['From'] = self.config['from_email']
            msg['To'] = self.config['to_email']
            
            # ãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡
            msg.attach(MIMEText(self.results['report'], 'plain', 'utf-8'))
            
            # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«
            attachments = [
                ('network_filtered.png', self.results['network_path']),
                ('wordcloud_filtered.png', self.results['wordcloud_path'])
            ]
            
            for filename, filepath in attachments:
                if os.path.exists(filepath):
                    with open(filepath, 'rb') as f:
                        part = MIMEBase('application', 'octet-stream')
                        part.set_payload(f.read())
                        encoders.encode_base64(part)
                        part.add_header('Content-Disposition', f'attachment; filename="{filename}"')
                        msg.attach(part)
            
            # é€ä¿¡
            with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:
                smtp.login(self.config['from_email'], self.config['app_password'])
                smtp.send_message(msg)
            
            print("ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œäº†ï¼")
            
        except Exception as e:
            print(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

# å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    miner = AdvancedTextMiner()
    miner.process_files()
